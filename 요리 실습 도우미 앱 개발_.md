

# **AI 컬리너리 코치: 실시간 시각적 피드백 기반 요리 실습 앱 개발을 위한 종합 전략 보고서**

**작성자:** 수석 기술 전략가 및 AI 제품 개발 컨설턴트

---

### **서론: 레시피를 넘어선 새로운 패러다임, 양방향 기술 습득**

본 보고서는 사용자가 제안한 혁신적인 AI 기반 요리 코칭 애플리케이션의 개념을 심층적으로 분석하고, 아이디어를 성공적인 제품으로 전환하기 위한 포괄적인 기술 및 사업 전략을 제시하는 것을 목표로 합니다. 사용자의 핵심 아이디어는 농구 연습 앱 '홈코트(HomeCourt)'의 성공 모델을 요리 분야에 적용하는 것입니다. 이는 단순히 정보를 제공하는 기존의 방식을 넘어, 사용자의 실제 행동에 실시간으로 피드백을 제공하여 기술 습득 과정을 근본적으로 바꾸는 새로운 패러다임을 제시합니다.

#### **"홈코트" 유추: 기술 코칭의 핵심 가치**

홈코트의 성공은 물리적 기술에 대한 실시간 정량적, 정성적 피드백을 제공함으로써 독자적인 연습을 데이터 기반의 가이드 경험으로 전환시킨 능력에 있습니다.1 이 앱은 스마트폰 카메라를 통해 사용자의 슛 동작, 드리블 속도 등을 분석하고 즉각적인 교정 가이드를 제공합니다. 이러한 핵심 가치, 즉 '실시간 피드백 루프'를 요리 기술이라는 새로운 영역에 적용하는 것이 본 프로젝트의 출발점입니다. 사용자는 더 이상 레시피를 수동적으로 따라 하는 존재가 아니라, AI 코치의 능동적인 지도를 받는 훈련생이 됩니다.

#### **비전: 수동적 지시에서 능동적 코칭으로**

현재 시장에 출시된 대부분의 요리 앱이나 VOD 콘텐츠는 정보를 수동적으로 전달하는 데 그칩니다. 사용자는 전문가의 시연을 보고 따라 할 뿐, 자신의 동작이 올바른지, 레시피의 미묘한 뉘앙스를 제대로 구현하고 있는지 알 방법이 없습니다. 본 보고서에서 제안하는 앱은 이러한 '수동적 지시'의 한계를 극복하고 '능동적 코칭'이라는 새로운 경험을 제공하고자 합니다. 이 앱의 AI는 "양파를 다지세요"라고 말하는 데 그치지 않고, 사용자의 칼질을 지켜보며 "손가락을 더 안으로 마세요" 또는 "이 레시피에는 더 잘게 다져야 합니다"와 같이 구체적이고 개인화된 피드백을 제공하는 것을 목표로 합니다.

#### **보고서 로드맵**

본 보고서는 총 4개의 섹션으로 구성됩니다. 각 섹션은 아이디어의 시장성 검증부터 핵심 기술 분석, 구체적인 시스템 설계, 그리고 단계별 개발 로드맵에 이르기까지, 성공적인 제품 개발을 위한 필수적인 요소들을 체계적으로 다룹니다.

* **섹션 1:** 디지털 요리 보조 시장 환경을 분석하여 '지도 공백(Guidance Gap)'을 정의하고, 이를 통해 명확한 시장 기회를 입증합니다.  
* **섹션 2:** AI 코치의 두뇌와 눈이 될 핵심 기술, 즉 컴퓨터 비전(CV)과 비전-언어 모델(VLM)의 역할과 기술적 과제를 심층적으로 탐구합니다.  
* **섹션 3:** 실시간 성능과 사용자 프라이버시를 보장하기 위한 온디바이스(On-device) 중심의 시스템 아키텍처 청사진을 제시합니다.  
* **섹션 4:** 거대한 비전을 실현 가능한 단계로 나누어, MVP(최소 기능 제품)부터 완전한 플랫폼으로 확장하기 위한 전략적 개발 로드맵을 제안합니다.

이 보고서를 통해 사용자의 혁신적인 비전을 현실로 만들 구체적이고 실행 가능한 전략을 얻게 될 것입니다.

---

### **섹션 1: 디지털 컬리너리 어시스턴트 시장 환경: '지도 공백'의 발견**

이 섹션에서는 현재 디지털 요리 보조 도구 시장을 면밀히 분석하여 제안된 앱의 사업 기회를 검증하고, 독자적인 가치 제안을 정의하며, 경쟁 생태계를 파악합니다. 분석 결과, 시장은 레시피 추천과 같은 정보 제공 기능에는 성숙해 있지만, 실시간 기술 코칭이라는 영역에는 명백한 공백이 존재함을 확인할 수 있습니다.

#### **1.1 현 세대의 AI: 레시피 사서 및 플래너로서의 역할**

* **기능 개요:** 현재 AI 요리 앱 시장은 레시피 생성 및 식단 계획 기능이 지배하고 있습니다. **ChefGPT** 3,  
  **Cook AI** 5,  
  **AI Chef Assistant** 6,  
  **Magic Chef** 7와 같은 앱들이 대표적입니다. 이들 앱의 핵심 기능은 사용자가 보유한 재료 목록과 같은 텍스트나 간단한 이미지를 입력받아 텍스트 기반의 레시피를 생성하는 것입니다.  
* **기술적 구현:** 이러한 앱들은 주로 요리 데이터에 미세 조정된(fine-tuned) 표준 거대 언어 모델(LLM)의 텍스트 생성 능력에 의존합니다.8  
  **Cook AI**나 **AI Food Scan**과 같이 일부 앱은 사진 속 재료를 식별하기 위해 기본적인 컴퓨터 비전 기술을 통합했지만 5, 이는 실시간 비디오 처리가 아닌 단일 프레임 분석에 불과합니다.  
* **사용자 경험:** 실제 요리 과정에서 사용자와의 상호작용은 거의 없습니다. AI의 역할은 레시피가 제공되는 순간 종료됩니다. **OLI**와 같은 앱은 커뮤니티 기반의 학습 요소를 추가하려 시도하지만 11, 여전히 실시간 개인화된 지도는 부재합니다.

#### **1.2 '지도 공백(Guidance Gap)': 명확한 시장 기회**

* **공백의 정의:** 본 보고서는 '지도 공백'을 요리 과정에서 실시간 시각적, 교정적 피드백이 부재한 상태로 정의합니다. 기존 앱들은 "양파를 다지세요"라고 지시할 수는 있지만, 사용자의 동작을 보고 "칼을 쥔 자세가 위험합니다" 또는 "이 레시피에는 주사위 모양이 너무 큽니다"라고 교정해 줄 수는 없습니다. 이것이 바로 제안된 앱이 해결하고자 하는 핵심 문제입니다.  
* **사용자 리뷰를 통한 증거:** 기존 앱에 대한 사용자 피드백은 이러한 공백을 명확히 보여줍니다. 리뷰에서는 AI가 생성한 레시피의 모호함, 누락된 단계, 심지어 식품 안전 위험까지 지적하는 경우가 많습니다.7 이는 사용자들이 더 신뢰할 수 있고 상호작용적인 지도를 원한다는 명백한 증거입니다. 예를 들어, 사진으로부터 레시피를 생성하는 SideChef의 AI 기능은 영감을 주는 도구로서는 유용하지만, 미묘한 차이를 파악하는 데 어려움을 겪으며 "확신이 없을 때는 임의로 만들어내는 경향이 있다"는 평가를 받습니다.12

#### **1.3 인접 혁신: 하드웨어 및 AR 개념**

* **하드웨어 통합 시스템:** **GE Kitchen Hub** 13나  
  **upliance.ai** 14와 같은 스마트 주방 기기는 카메라와 AI를 기기에 직접 통합합니다. GE Kitchen Hub는 오븐 내부 카메라와 AI를 이용해 음식의 익힘 정도를 감지합니다. 이는 주방 환경에서 시각 AI에 대한 시장의 관심을 보여주지만, 특정 하드웨어에 종속된 폐쇄적이고 고가의 생태계라는 한계를 가집니다.  
* **증강현실(AR) 프로토타입:** AR 요리 보조 시스템에 대한 연구들은 15 시각적 지시를 현실 세계에 겹쳐 보여주는 기술에 대한 학술적, 연구개발적 관심을 증명합니다. 그러나 이러한 시스템들은 종종 AR 헤드셋과 같은 특수 하드웨어를 요구하며 아직 주류 상용화에는 이르지 못했습니다. 이는 강화된 시각적 지도에 대한 수요를 검증하는 동시에, 보다 접근성 높은 스마트폰 기반 솔루션을 제안하는 본 프로젝트의 전략적 이점을 부각시킵니다.

#### **1.4 핵심 결론 및 전략적 포지셔닝**

분석을 종합하면, 현재 시장은 텍스트 기반 레시피 생성 분야에서는 성숙했지만 실시간 시각 코칭 분야에서는 초기 단계에 머물러 있습니다. 이는 명백한 '블루오션' 기회가 존재함을 의미합니다. 따라서 제안된 앱은 단순히 또 다른 요리 앱이 아니라, '컬리너리 기술 개발 플랫폼'이라는 새로운 카테고리를 창출할 잠재력을 가집니다. 주요 경쟁 상대는 다른 레시피 앱이 아니라 전통적인 요리 교실이나 VOD 콘텐츠이며, 이 앱의 가치 제안은 개인화된 양방향 코칭을 훨씬 저렴한 비용으로 제공하는 것입니다.

이러한 시장 분석은 제안된 아이디어가 단순한 상상이 아니라, 기술적 진보와 시장의 요구가 만나는 지점에 정확히 위치하고 있음을 보여줍니다. 현재 시장은 "무엇을 만들까?"라는 *요리 전(pre-cooking)* 단계의 문제 해결에 집중하고 있습니다. 반면, 제안된 앱은 "어떻게 잘 만들까?"라는 *요리 중(during-cooking)* 단계의 문제를 공략합니다. GE와 같은 대기업이나 AR 연구 분야에서 이미 '요리 중' 단계를 다음 격전지로 인식하고 고가의 하드웨어 솔루션을 탐색하고 있다는 점 13은 이 문제의 중요성을 방증합니다. 이는 소프트웨어만으로 접근 가능한 솔루션을 제공하려는 본 프로젝트에 전략적인 '스위트 스팟'을 형성해 줍니다. 시장은 주방에서의 AI 지원 가치에 대해 이미 교육받았지만, 아직 아무도 실시간 코칭 문제에 대한 확장 가능하고 저렴한 솔루션을 제공하지 못했습니다.

**표 1: AI 기반 컬리너리 도구 경쟁 분석**

| 제품명 | 주요 기능 | 핵심 AI 기술 | 피드백 메커니즘 | 핵심 한계점 | 수익 모델 |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **ChefGPT** 3 | 레시피 생성, 식단 계획 | LLM 기반 텍스트 생성 | 텍스트 전용 | 실시간 지도 부재, 요리 중 상호작용 없음 | 부분 유료화/구독 |
| **Cook AI** 5 | 재료 기반 레시피 생성 | 단일 프레임 재료 인식, LLM | 텍스트 전용 | 실시간 지도 부재, 정적 이미지 분석 한계 | 부분 유료화/구독 |
| **OLI** 11 | AI 레시피, 커뮤니티 학습 | LLM 기반 단계별 지시 | 요리 후 커뮤니티 피드백 | 실시간 교정 기능 부재 | 인앱 구매 |
| **GE Kitchen Hub** 13 | 스마트 오븐, 레시피 가이드 | 내장 카메라 기반 AI 익힘 정도 감지 | 하드웨어 상태 알림 | 고가의 특정 하드웨어 필요, 제한된 기능 | 하드웨어 판매 |
| **upliance.ai** 14 | 자동 요리 기기 | 내장 레시피 및 자동 제어 알고리즘 | 기기 알림음 | 고가의 특정 하드웨어 필요, 수동 기술 학습 불가 | 하드웨어 판매 |
| **제안된 앱** | **실시간 요리 기술 코칭** | **실시간 객체/행동 인식, VLM 기반 피드백** | **실시간 시각/음성/텍스트 교정** | \- | 부분 유료화/구독 |

---

### **섹션 2: 핵심 기술 – 컬리너리 코치의 AI 엔진**

이 섹션에서는 제안된 앱의 AI 코칭 기능을 구현하는 데 필요한 핵심 기술들을 심층적으로 분석합니다. 사용자가 막연하게 생각한 'LLM 기반'이라는 개념을 넘어, 실제로는 여러 전문 AI 모델이 유기적으로 결합된 복잡한 시스템이 필요함을 명확히 합니다. 각 기술의 역할, 구현 난이도, 그리고 극복해야 할 과제들을 구체적으로 정의합니다.

#### **2.1 코치의 눈: 주방 지능을 위한 컴퓨터 비전(CV)**

컴퓨터 비전은 스마트폰 카메라를 통해 사용자의 요리 환경과 행동을 '보는' 역할을 담당하는 foundational layer입니다. 이는 단순히 이미지를 인식하는 것을 넘어, 복잡하고 동적인 주방 상황을 실시간으로 이해하는 것을 목표로 합니다.

* **기반 계층: 객체 및 조리도구 인식**  
  * 시스템은 먼저 주방이라는 무대 위의 '행위자'들, 즉 식재료, 냄비나 프라이팬 같은 조리기구, 칼이나 주걱 같은 조리도구를 식별해야 합니다. 이를 위해 YOLOv5와 같은 강력한 객체 탐지 모델의 활용이 필수적입니다.19 다행히 주방용품 21 및 식재료 25에 특화된 공개 데이터셋이 존재하여 모델 훈련의 시작점을 제공합니다.  
  * **도전 과제:** 하지만 주방 환경은 가려짐(occlusion), 시시각각 변하는 조명, 반사되는 표면, 형태가 일정하지 않은 식재료 등 시각적 복잡성이 매우 높아 객체 인식을 결코 간단하지 않은 문제로 만듭니다.30 실제로 냉장고 속 정적인 재료를 사진으로 인식하는 기존 앱들조차 정확도에 대한 사용자 불만이 제기된 바 있습니다.7  
* **핵심 기술: 세분화된 행동 인식(Fine-Grained Action Recognition)**  
  * 이것이 바로 앱의 독창적인 가치가 탄생하는 지점입니다. AI는 칼과 양파를 보는 것만으로는 부족하며, '양파를 다지는' *행동* 자체를 인식해야 합니다.  
  * 여기서 중요한 것은 '자르기'와 같은 포괄적인 행동(coarse-grained action)과 '다지기', '채썰기', '깍둑썰기'와 같은 세분화된 행동(fine-grained action)을 구분하는 능력입니다. 의미 있는 피드백을 제공하기 위해서는 후자의 인식이 필수적입니다.31  
  * 이를 위해서는 비디오의 시간적 순서(temporal sequence)를 분석할 수 있는 모델이 필요합니다. Temporal Shift Module (TSM)과 같은 아키텍처가 논의될 수 있으며 31,  
    **EPIC-KITCHENS** 35,  
    **MPII Cooking Activities** 37,  
    **YouCookII** 39와 같은 도메인 특화 데이터셋의 중요성은 아무리 강조해도 지나치지 않습니다. 이 데이터셋들은 행동 구간, 동사/명사 레이블 등 세분화된 행동 인식을 훈련시키는 데 필수적인 주석(annotation)이 달린 비디오 영상을 제공합니다.  
* **차세대 과제: 행동 품질 평가 및 상태 인식**  
  * 행동을 인식하는 것을 넘어, 궁극적인 목표는 그 행동의 *품질*을 평가하는 것입니다. 이는 칼의 각도, 다진 결과물의 균일성, 젓는 속도 등 미묘한 시각적 단서를 분석하는 것을 포함합니다.  
  * 또한, 요리 단계의 목표인 식재료의 *상태 변화*를 인식하는 것도 중요합니다. 예를 들어, 물이 끓기 시작하는 시점, 버터가 녹은 상태, 양파가 투명해진 정도를 파악하는 것입니다.19 이는 단순한 분류 문제를 넘어 연속적인 상태 변화를 인식하는 활발한 연구 분야입니다.

#### **2.2 코치의 두뇌: 실시간 지도를 위한 비전-언어 모델(VLM)**

사용자가 언급한 'LLM'은 이 시스템의 '두뇌' 역할을 하지만, 그 자체만으로는 시각 정보를 처리할 수 없습니다. 필요한 것은 시각 정보와 언어 정보를 통합적으로 처리하는 특화된 AI, 즉 비전-언어 모델(VLM)입니다.

* **시각과 언어의 연결:** 표준 LLM은 '볼' 수 없습니다. 이 프로젝트의 핵심 기술은 시각 정보를 처리하는 비전 인코더와 텍스트를 처리/생성하는 언어 모델을 통합한 \*\*비전-언어 모델(VLM)\*\*입니다.43  
  **CLIP**과 같은 모델은 이미지와 텍스트를 공유된 임베딩 공간에 매핑하여 둘 사이의 연관성을 학습하는 기반 기술을 제공합니다.47  
* **파이프라인 내 VLM의 역할:** VLM은 '해석자' 역할을 합니다. 컴퓨터 비전 모델로부터 {"object": "knife", "action": "chopping", "target": "onion", "speed": "120/min"}와 같은 구조화된 데이터를 입력받고, 동시에 앱 로직으로부터 "양파를 잘게 다지세요"라는 레시피 단계를 전달받습니다. VLM의 임무는 이 둘을 비교하여 자연어 형태의 평가와 피드백을 생성하는 것입니다.  
* **고품질 피드백 생성: 프롬프트 엔지니어링 및 미세 조정**  
  * **프롬프트 엔지니어링:** 이 과업을 성공적으로 수행하기 위해서는 고도의 프롬프트 엔지니어링 기술이 필수적입니다. "당신은 도움이 되면서도 정확한 미슐랭 스타 셰프입니다..."와 같은 \*\*역할 기반 프롬프트(Role-based Prompting)\*\*나, "1단계: 사용자의 행동을 식별하라. 2단계: 레시피의 요구 사항과 비교하라. 3단계: 불일치가 있다면 건설적인 교정 방안을 제시하라."와 같은 **사고의 연쇄 프롬프트(Chain-of-Thought Prompting)** 기법을 활용해야 합니다.50  
  * **명령어 미세 조정(Instruction Fine-Tuning):** 신뢰성 있고 도메인에 특화된 응답을 생성하기 위해, 기반 VLM은 추가적인 미세 조정 과정을 거쳐야 합니다. 이는 (시각적 맥락, 레시피 지시) \-\> (피드백) 형태의 데이터셋을 구축하여 모델을 훈련시키는 과정입니다. 이 과정을 통해 모델의 행동을 '요리 코칭'이라는 특정 작업에 맞게 정렬(align)시킬 수 있습니다.53  
* **실시간 상호작용의 도전 과제:** 표준 VLM은 대부분 순차적(turn-based)으로 작동합니다. 하지만 이 앱은 연속적인 스트리밍 상호작용을 요구합니다. 이는 AI가 응답을 생성하는 *동시에* 새로운 시각 정보를 처리하여 즉석에서 피드백을 수정할 수 있어야 함을 의미합니다. 이는 단순한 시각적 질의응답(VQA) 앱과 본 프로젝트를 구분 짓는 중요한 기술적 허들로, 실시간 VLM에 대한 최신 연구 동향을 주시해야 합니다.56

#### **2.3 AI의 감각 강화: 데이터 증강 및 합성 데이터**

* **데이터 증강(Data Augmentation):** 컴퓨터 비전 모델이 다양한 가정의 주방 환경에 강건하게 대응하기 위해서는 공격적인 데이터 증강 기법이 필수적입니다. 색상 변화, 회전과 같은 전통적인 기법뿐만 아니라, 증강 효과에 부드러운 시간적 변화를 적용하는 **DynaAugment**와 같은 비디오 특화 고급 기법을 사용해야 합니다.26  
* **합성 데이터 생성(Synthetic Data Generation):** 현실에서 발생할 수 있는 모든 요리 변수를 촬영하는 것은 불가능에 가깝습니다. 따라서 **Unreal Engine**이나 **Unity**와 같은 게임 엔진을 사용하여 합성 훈련 데이터를 생성하는 방안을 제안합니다.62 이 접근법은 완벽하고 자동으로 생성된 주석을 제공하며, 냄비가 끓어 넘치는 것과 같은 드물거나 위험한 시나리오를 통제된 환경에서 생성할 수 있게 하여 '시뮬레이션-현실 간극(simulation-reality gap)'을 메워줍니다.66

이처럼 AI 코칭 시스템은 단일 AI가 모든 것을 처리하는 구조가 아닙니다. 이는 각자의 전문 분야를 가진 AI 모델들이 협력하는 '위원회'와 같은 복잡한 파이프라인입니다. 객체 탐지 모델 20, 세분화된 행동 인식 모델 31, 상태 인식 모델 41, 그리고 이 모든 정보를 종합하여 판단을 내리는 VLM 감독관 43이 각자의 역할을 수행합니다. 따라서 개발 전략의 핵심은 단순히 '어떤 LLM을 사용할까?'가 아니라, '어떻게 이 다중 모델 파이프라인을 효율적으로 구축하고 모바일 기기에서 실시간으로 구동시킬 것인가?'라는 질문에 답하는 것이어야 합니다. 이는 프로젝트의 기술적 방향을 근본적으로 재정의합니다.

**표 2: 컬리너리 AI 코치 훈련을 위한 공개 데이터셋**

| 데이터셋 이름 | 콘텐츠 유형 | 주석 유형 | 규모 | 주요 사용 사례 | 출처 |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **EPIC-KITCHENS-100** | 1인칭 시점 비디오 | 행동 구간 (동사/명사), 객체 바운딩 박스 | 100시간, 700개 비디오 | 세분화된 행동 인식, 객체-손 상호작용 | 35 |
| **MPII Cooking Activities** | 3인칭 시점 비디오 | 행동 구간, 신체 포즈 | 8시간 이상, 44개 비디오, 65개 행동 클래스 | 세분화된 행동 인식, 포즈 추정 | 37 |
| **YouCookII** | 3인칭 시점 비디오 | 시간적 구간에 대한 절차적 텍스트 설명 | 176시간, 2,000개 비디오 | 행동 분할, 비디오 캡셔닝 | 39 |
| **Food-101 / Food-256** | 정적 이미지 | 이미지 레벨 레이블 (음식 종류) | 10만+ 이미지, 101/256개 클래스 | 식재료 및 완성된 음식 인식 | 26 |
| **Kitchenware Classifier** | 정적 이미지 | 이미지 레벨 레이블 (컵, 포크 등 6종) | Kaggle 데이터셋 | 조리도구 인식 | 21 |
| **COM Kitchens** | 고정 시점 비디오 | 시각적 행동 그래프, 바운딩 박스 | 40시간, 145개 비디오 | 절차적 비디오 이해, 온라인 레시피 검색 | 40 |

---

### **섹션 3: 시스템 아키텍처 및 온디바이스 배포**

이 섹션에서는 앞서 논의된 기술적 요구사항들을 구체적인 아키텍처 청사진으로 전환합니다. 실시간 성능과 사용자 프라이버시를 최우선으로 고려하여, 모든 핵심 AI 처리가 사용자의 기기 내에서 이루어지는 '온디바이스(On-device)' 전략을 중심으로 설계합니다.

#### **3.1 고수준 시스템 아키텍처: 다계층 접근 방식**

본 시스템은 실시간 피드백 루프의 효율성을 극대화하기 위해 명확하게 구분된 여러 계층으로 구성됩니다. 데이터 흐름은 다음과 같습니다.

* **시각 처리 계층 (온디바이스):** 스마트폰에서 직접 실행되는 이 계층은 카메라 인터페이스를 통해 실시간 비디오 스트림을 입력받습니다. 여기에는 객체 탐지, 행동 인식 등 앞서 논의된 컴퓨터 비전 모델들이 포함됩니다. 이 계층의 최종 출력물은 주방의 상황을 설명하는 구조화된 메타데이터(예: JSON 객체)의 연속적인 스트림입니다.46  
* **맥락 통합 및 추론 계층 (온디바이스):** 비전-언어 모델(VLM)이 상주하는 곳입니다. 시각 처리 계층으로부터 메타데이터 스트림을, 앱 로직으로부터 현재 수행해야 할 레시피 단계를 전달받습니다. 이 두 정보를 비교 분석하여 "피드백 의도(feedback intent)"를 생성하는 핵심적인 추론 과정을 담당합니다.46  
* **언어 처리 및 UI 계층 (온디바이스):** 추론 계층에서 생성된 피드백 의도(예: {"feedback\_type": "correction", "issue": "knife\_grip", "suggestion": "tuck\_fingers"})를 받아 사용자가 인지할 수 있는 형태로 변환합니다. 이는 화면에 표시되는 텍스트 오버레이, 음성 안내, 또는 햅틱 진동과 같은 사용자 인터페이스(UI) 출력으로 나타납니다.  
* **콘텐츠 및 백엔드 계층 (클라우드):** 전통적인 클라우드 서버의 역할은 실시간 AI 처리 루프에서 벗어나 있습니다. 이 계층은 사용자 계정 관리, 전문가 셰프의 시연 VOD 콘텐츠 전송, 레시피 데이터베이스 관리 등 비실시간 작업을 처리합니다.67

#### **3.2 모바일 우선, 온디바이스 전략: 성능과 프라이버시**

모든 핵심 AI 추론을 사용자의 기기에서 직접 수행하는 것은 이 프로젝트의 성패를 좌우하는 매우 중요한 전략적 결정입니다.

* **왜 온디바이스인가?:**  
  1. **지연 시간(Latency):** 실시간 피드백은 비디오를 클라우드로 전송하고 결과를 다시 받는 과정에서 발생하는 수 초의 지연 시간으로는 불가능합니다. 즉각적인 반응성을 위해서는 모든 처리가 기기 내에서 완결되어야 합니다.  
  2. **프라이버시(Privacy):** 사용자는 자신의 주방을 지속적으로 촬영하는 영상이 외부 서버로 전송되는 것에 대해 심각한 프라이버시 우려를 가질 수 있습니다. 온디바이스 처리는 데이터가 기기 밖으로 나가지 않음을 보장하여 사용자의 신뢰를 얻는 데 결정적입니다.  
  3. **비용(Cost):** 대규모 사용자를 대상으로 비디오 스트리밍 및 클라우드 기반 AI 처리를 제공하는 것은 막대한 운영 비용을 발생시킬 수 있습니다. 온디바이스 모델은 이러한 비용을 근본적으로 제거합니다.68  
* **온디바이스 AI의 과제:** 복잡한 여러 AI 모델을 한정된 자원을 가진 모바일 기기에서 실행하는 것은 상당한 최적화 노력을 요구합니다. 모델 양자화(quantization), 가지치기(pruning)와 같은 기술을 통해 모델의 크기를 줄이고, sLLM(소형 언어 모델)과 같이 더 작고 효율적인 아키텍처를 채택하여 모바일의 연산 및 메모리 제약 조건 내에서 작동하도록 해야 합니다.68

#### **3.3 플랫폼별 구현 청사진**

각 모바일 운영체제는 고유의 최적화된 머신러닝 프레임워크를 제공합니다. 플랫폼별 최적의 기술 스택을 활용하는 것이 중요합니다.

* **iOS (Apple 기기):**  
  * **핵심 프레임워크:** 권장되는 기술 스택은 **Core ML**과 **Vision** 프레임워크입니다.71 이들은 Apple Silicon(CPU, GPU, 특히 Neural Engine)에 고도로 최적화되어 있어 최고의 성능을 보장합니다.  
  * **구현 방식:** AVCaptureSession을 사용하여 비디오 버퍼를 가져오고, Vision 프레임워크를 통해 전처리한 후, 일련의 MLModel 추론을 통해 객체 탐지 및 행동 인식을 실시간으로 수행하는 방식을 상세히 기술할 수 있습니다.73  
* **Android 기기:**  
  * **핵심 프레임워크:** 권장되는 기술 스택은 **MediaPipe**와 \*\*TensorFlow Lite (TFLite)\*\*입니다.76 MediaPipe는 효율적인 크로스플랫폼 인식 파이프라인을 구축하는 데 매우 강력하며 78, TFLite는 안드로이드에서 최적화된 모델을 배포하는 표준입니다.80  
  * **구현 방식:** MediaPipe가 제공하는 **Gesture Recognizer** 81나 3D 객체 탐지를 위한  
    **Objectron** 83과 같은 사전 구축된 구성 요소를 활용하고, 특정 요리 행동 및 재료 인식을 위해 훈련된 맞춤형 TFLite 모델을 통합하는 방식을 설명할 수 있습니다. 특히, 새로운 MediaPipe LLM Inference API는 VLM 구성 요소를 온디바이스에서 실행하는 핵심적인 기술입니다.84

이 아키텍처의 설계 원칙은 명확합니다. 앱의 핵심 가치인 실시간 피드백 루프를 그 어떤 것보다 우선시하는 것입니다. 이는 전통적인 앱 개발 패러다임, 즉 모든 무거운 연산을 백엔드로 보내는 방식과는 정반대입니다. 수백 밀리초 내에 사용자의 행동에 대한 피드백이 이루어져야 하는 본 앱의 특성상, CV 모델에서 VLM을 거쳐 UI에 피드백이 표시되기까지의 전 과정이 기기 내에서 폐쇄 루프(closed-loop)로 실행되어야 합니다.69 이 '온디바이스 우선' 원칙은 기술 스택 선택부터 모델 최적화 전략에 이르기까지 모든 기술적 결정을 지배하는 중심 기둥이며, 동시에 '프라이버시 보호'와 '오프라인 작동 가능'이라는 강력한 마케팅 메시지를 제공하는 기반이 됩니다.

---

### **섹션 4: 단계별 개발 로드맵: MVP에서 완전한 비전까지**

이 마지막 섹션에서는 거대하고 야심 찬 비전을 관리 가능하고 가치를 전달하는 단계들로 나누어, 프로젝트의 리스크를 줄이고 성공 가능성을 높이는 실행 가능한 전략 계획을 제시합니다. 이 로드맵은 "레시피는 곧 커리큘럼이다"라는 핵심 원칙에 따라 기술 개발이 콘텐츠와 긴밀하게 연계되도록 설계되었습니다.

#### **4.1 1단계 \- 최소 기능 제품(MVP): "절차 확인기 (The Procedural Checker)"**

* **목표:** 핵심적인 엔드투엔드 기술 파이프라인을 검증하고, 기본 컨셉이 기술적으로 실현 가능하며 사용자에게 매력적인지 증명합니다.  
* **사용자 대면 기능:**  
  * 단 하나의 간단한 레시피 (예: 스크램블 에그)를 제공합니다.  
  * 전문가 셰프의 VOD는 별도의 디스플레이에서 재생됩니다.  
  * 앱은 카메라를 사용하여 *크고 명확하게 구분되는 주요 단계들*의 진행 상황을 추적합니다.  
* **기술 초점:**  
  * **컴퓨터 비전(CV):** 핵심 재료(계란, 우유, 팬, 주걱)에 대한 안정적인 객체 탐지와 기본 상태 인식(팬이 가열 중인지, 계란이 팬에 들어갔는지 등)을 구현합니다. 이 단계에서는 아직 세분화된 행동 인식은 포함되지 않습니다.  
  * **비전-언어 모델(VLM)/LLM:** 사전 훈련된 VLM을 간단한 프롬프트 엔지니어링과 함께 사용합니다. 피드백은 질적인 평가가 아닌 절차적인 확인에 중점을 둡니다. 예를 들어, "좋아요, 계란을 넣으셨네요. 다음은 부드럽게 저어주세요."와 같은 안내나, 현재 단계 옆에 체크 표시가 나타나는 수준입니다.  
* **성공 지표:** 시스템이 레시피의 사건 순서를 90% 이상의 정확도로 올바르게 식별합니다. 사용자들이 이 경험을 방해받는다고 느끼지 않고 유용하다고 평가합니다.

#### **4.2 2단계 \- 핵심 혁신: "기술 코치 (The Technique Coach)"**

* **목표:** 앱의 핵심 차별화 요소인 세분화된 행동에 대한 피드백 기능을 도입합니다.  
* **사용자 대면 기능:**  
  * 특정하고 가르칠 만한 기술이 포함된 3\~5개의 레시피로 확장합니다 (예: 다지기가 필요한 레시피, 접기(folding) 기술이 필요한 레시피).  
  * AI가 이제 *제한된 종류*의 행동에 대해 실시간 교정 피드백을 제공합니다. 예를 들어, "채썰기 할 때 손가락을 안으로 마세요," 또는 "반죽을 접는 동작이 너무 과격합니다."와 같은 피드백입니다.  
* **기술 초점:**  
  * **컴퓨터 비전(CV):** 특정 기술(예: 칼질 기술)에 대한 최초의 맞춤형 **세분화된 행동 인식** 모델을 훈련하고 배포합니다. 이를 위해서는 EPIC-KITCHENS와 같은 데이터셋을 활용한 광범위한 작업이 필요합니다.35  
  * **비전-언어 모델(VLM)/LLM:** (행동 비디오 클립, 피드백 텍스트) 쌍으로 구성된 데이터셋을 구축하여 VLM에 대한 \*\*명령어 미세 조정(instruction fine-tuning)\*\*을 시작합니다. 이를 통해 더 미묘하고 도움이 되는 코칭 조언을 생성할 수 있게 됩니다.53  
* **성공 지표:** 사용자들이 코칭받은 기술에 대해 측정 가능한 실력 향상을 보고합니다. AI 피드백이 정확하고 건설적이라고 평가받습니다.

#### **4.3 3단계 \- 플랫폼 확장: "컬리너리 아카데미 (The Culinary Academy)"**

* **목표:** 콘텐츠 라이브러리와 AI의 능력을 확장하여 포괄적인 학습 플랫폼으로 발전시킵니다.  
* **사용자 대면 기능:**  
  * 레시피와 코칭 기술의 수를 급격히 늘립니다.  
  * 시간에 따른 기술 향상도를 추적하는 사용자 프로필을 도입합니다.  
  * 핸즈프리 제어를 위한 음성 상호작용("AI, 마지막 단계 다시 말해줘") 기능을 추가합니다.4  
* **기술 초점:**  
  * **자동화:** 새로운 레시피를 추가하고 새로운 행동에 대해 AI를 훈련시키는 프로세스를 간소화하는 파이프라인을 구축합니다. 여기에는 LLM을 사용하여 레시피 텍스트를 분석하고 모델링해야 할 핵심 단계와 행동을 식별하는 작업이 포함될 수 있습니다.85  
  * **개인화:** AI가 사용자의 기술 수준에 적응하기 시작하여, 숙련된 사용자에게는 더 고급 팁을, 초보자에게는 더 기본적인 지도를 제공합니다. VLM의 이력(history) 기능이 여기서 중요한 역할을 합니다.4  
* **성공 지표:** 높은 사용자 유지율 및 참여도. 사용자당 완료된 레시피 수의 증가.

#### **4.4 미래 비전: 증강 및 소셜 주방**

궁극적인 제품 비전을 안내하기 위해 장기적인 가능성을 간략하게 탐색합니다.

* **증강현실(AR):** 시각적 가이드를 도마나 팬 위에 직접 투사합니다.15  
* **소셜 통합:** 사용자가 자신의 진행 상황을 공유하거나 특정 기술에 대해 친구와 '챌린지'를 할 수 있도록 하여, OLI와 같은 앱의 커뮤니티 아이디어를 발전시킵니다.11  
* **하드웨어 통합:** 스마트 가전제품 제조업체와의 잠재적인 미래 파트너십을 모색합니다.13

이러한 단계별 접근법의 핵심은 기술 개발을 각 단계의 명확한 사용자 가치에 직접 연결하여 프로젝트의 불확실성을 체계적으로 줄여나가는 것입니다. 1단계의 목표는 '행동 인식기 만들기'가 아니라 '스크램블 에그 레시피의 절차를 성공적으로 확인하는 시스템 만들기'가 됩니다. 마찬가지로 2단계의 목표는 '양파 수프 레시피 모듈에서 다지기 기술을 성공적으로 코칭할 수 있는 행동 인식기 구축하기'가 됩니다. 이처럼 콘텐츠 중심의 로드맵은 개발팀에 명확한 목표를 제공하고, 각 단계마다 실질적인 성과물을 만들어내며 프로젝트를 성공으로 이끄는 가장 확실한 경로입니다.

**표 3: 단계별 개발 및 기술 로드맵**

| 단계 | 주요 목표 | 핵심 사용자 기능 | 핵심 기술 초점 | 주요 성공 지표 | 예상 복잡도 |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **1: MVP** | 핵심 파이프라인 검증 | 1개의 간단한 레시피, 절차적 확인 | 객체 탐지, 기본 상태 인식 | 절차 정확도 \> 90% | 낮음 |
| **2: 기술 코치** | 세분화된 피드백 도입 | 3-5개의 기술 기반 레시피, 칼질 등에 대한 교정 피드백 | 세분화된 행동 인식, VLM 명령어 미세 조정 | 사용자 평가 피드백 정확도 \> 80% | 중간 |
| **3: 컬리너리 아카데미** | 콘텐츠 및 개인화 확장 | 50개 이상의 레시피, 음성 제어, 기술 추적 | 자동화된 콘텐츠 파이프라인, 개인화 알고리즘 | 사용자 유지율, 월별 완료 레시피 수 | 높음 |

---

### **결론 및 전략적 권고**

본 보고서는 제안된 AI 컬리너리 코치 앱이 상당한 시장 기회를 가지고 있으며, 기술적으로는 야심 차지만 실현 가능한 프로젝트임을 확인했습니다. 성공을 위해서는 다음과 같은 전략적 원칙을 반드시 준수해야 합니다.

* **핵심 발견 사항 요약:**  
  * **시장 기회:** 현재 시장에는 실시간 시각적 코칭이라는 명백한 '지도 공백'이 존재합니다.  
  * **기술적 현실:** 이 시스템은 단일 AI가 아닌, 여러 전문 AI 모델로 구성된 '위원회'와 같은 복잡한 파이프라인을 요구합니다.  
  * **아키텍처 원칙:** 실시간 성능과 프라이버시를 위해 모든 핵심 AI 처리는 '온디바이스'에서 이루어져야 합니다.  
  * **개발 접근법:** "레시피는 곧 커리큘럼"이라는 원칙하에 콘텐츠 중심의 단계별 개발 로드맵이 필수적입니다.  
* **최종 권고 사항:**  
  1. **'기술 개발' 틈새시장을 공략하라:** 앱의 포지셔닝과 마케팅을 단순한 레시피 앱이 아닌, 혁신적인 기술 습득 도구로 명확히 해야 합니다.  
  2. **데이터 확보 전략을 최우선으로 하라:** 주석이 달린 고품질 요리 비디오 데이터의 양과 질이 AI 코치의 성능을 결정하는 가장 중요한 단일 요소가 될 것입니다. 프로젝트 초기부터 EPIC-KITCHENS와 같은 공개 데이터셋을 적극적으로 활용하고, 장기적으로는 합성 데이터 생성 파이프라인 구축을 계획해야 합니다.  
  3. **다학제적 팀을 구성하라:** 성공을 위해서는 모바일 개발자뿐만 아니라 컴퓨터 비전, NLP/VLM 전문가, 그리고 '커리큘럼'(레시피 선택)과 피드백 설계를 이끌 요리 전문가가 반드시 포함된 팀이 필요합니다.  
  4. **MVP는 단일 플랫폼에 집중하라:** iOS(Core ML)와 Android(MediaPipe) 양쪽 모두에 대한 최적화는 복잡성이 높으므로, 첫 출시는 단일 플랫폼에 집중하여 프로젝트의 기술적 리스크를 우선적으로 해소한 후 확장해야 합니다.  
  5. **사용자 기대를 관리하라:** 각 개발 단계에서 AI의 능력을 투명하게 알려야 합니다. '절차 확인기'에서 '미슐랭 스타 코치'로 가는 여정은 길고 점진적입니다. 사용자 기대를 현명하게 관리하는 것이 충성도 높은 초기 사용자 커뮤니티를 구축하는 열쇠가 될 것입니다.

이상의 전략적 권고 사항들을 충실히 이행한다면, 제안된 AI 컬리너리 코치 앱은 단순한 아이디어를 넘어, 사람들이 가정에서 새로운 기술을 배우는 방식을 혁신하는 성공적인 제품으로 탄생할 수 있을 것입니다.

#### **참고 자료**

1. Top 10 Homecourt Alternatives & Competitors in 2025 \- G2, 6월 30, 2025에 액세스, [https://www.g2.com/products/homecourt/competitors/alternatives](https://www.g2.com/products/homecourt/competitors/alternatives)  
2. Top HomeCourt Alternatives in 2025 \- Slashdot, 6월 30, 2025에 액세스, [https://slashdot.org/software/p/HomeCourt/alternatives](https://slashdot.org/software/p/HomeCourt/alternatives)  
3. ChefGPT | AI Recipe Generator | Your AI-Powered Personal Chef, 6월 30, 2025에 액세스, [https://www.chefgpt.xyz/](https://www.chefgpt.xyz/)  
4. ChefGPT Explained: How Smart Cooking Apps Are Developed with AI \- Emizentech, 6월 30, 2025에 액세스, [https://emizentech.com/blog/chefgpt.html](https://emizentech.com/blog/chefgpt.html)  
5. Cook AI: Snap, Make & Taste on the App Store, 6월 30, 2025에 액세스, [https://apps.apple.com/us/app/cook-ai-snap-make-taste/id6504048524](https://apps.apple.com/us/app/cook-ai-snap-make-taste/id6504048524)  
6. AI Chef Assistant \- Apps on Google Play, 6월 30, 2025에 액세스, [https://play.google.com/store/apps/details?id=com.mea.aichefassistant](https://play.google.com/store/apps/details?id=com.mea.aichefassistant)  
7. Magic Chef AI Recipe Generator 12+ \- App Store, 6월 30, 2025에 액세스, [https://apps.apple.com/us/app/magic-chef-ai-recipe-generator/id6449253484](https://apps.apple.com/us/app/magic-chef-ai-recipe-generator/id6449253484)  
8. Be My Chef: AI-Powered Personalized Recipe Generator \- Deepgram, 6월 30, 2025에 액세스, [https://deepgram.com/ai-apps/be-my-chef](https://deepgram.com/ai-apps/be-my-chef)  
9. 6 Best AI Recipe Generators for Home Cooks \- Bootstrapped Ventures, 6월 30, 2025에 액세스, [https://bootstrapped.ventures/best-ai-recipe-generator/](https://bootstrapped.ventures/best-ai-recipe-generator/)  
10. AI Food Scan. Recipe Generator \- Apps on Google Play, 6월 30, 2025에 액세스, [https://play.google.com/store/apps/details?id=com.shw.platform.aicookassist.app](https://play.google.com/store/apps/details?id=com.shw.platform.aicookassist.app)  
11. OLI \- AI Cooking Assistant on the App Store, 6월 30, 2025에 액세스, [https://apps.apple.com/us/app/oli-ai-cooking-assistant/id6460936502](https://apps.apple.com/us/app/oli-ai-cooking-assistant/id6460936502)  
12. I Tried this AI Recipe Generator to Create a Restaurant Quality Meal at Home \- CNET, 6월 30, 2025에 액세스, [https://www.cnet.com/tech/services-and-software/i-tried-this-ai-recipe-generator-to-create-a-restaurant-quality-meal-at-home/](https://www.cnet.com/tech/services-and-software/i-tried-this-ai-recipe-generator-to-create-a-restaurant-quality-meal-at-home/)  
13. GE Appliances Debuts Cutting-Edge Digital Cooking Experience Powered by Artificial Intelligence, 6월 30, 2025에 액세스, [https://pressroom.geappliances.com/news/ge-appliances-debuts-cutting-edge-digital-cooking-experience-powered-by-artificial-intelligence](https://pressroom.geappliances.com/news/ge-appliances-debuts-cutting-edge-digital-cooking-experience-powered-by-artificial-intelligence)  
14. upliance.ai | Now everyone can cook-do-cook | AI cooking assistant, 6월 30, 2025에 액세스, [https://upliance.ai/](https://upliance.ai/)  
15. Augmented Reality Based Interactive Cooking Guide \- PMC, 6월 30, 2025에 액세스, [https://pmc.ncbi.nlm.nih.gov/articles/PMC9655470/](https://pmc.ncbi.nlm.nih.gov/articles/PMC9655470/)  
16. Augmented Reality Based Interactive Cooking Guide \- MDPI, 6월 30, 2025에 액세스, [https://www.mdpi.com/1424-8220/22/21/8290](https://www.mdpi.com/1424-8220/22/21/8290)  
17. How Effective Is Immersive AR Continental Food Course for Vocational Education? Analyzing Knowledge Gains and Learning Outcome E, 6월 30, 2025에 액세스, [https://www.ijiet.org/vol15/IJIET-V15N1-2225.pdf](https://www.ijiet.org/vol15/IJIET-V15N1-2225.pdf)  
18. CookAR: Affordance Augmentations in Wearable AR to Support Kitchen Tool Interactions for People with Low Vision \- madAbility: UW-Madison Accessibility Lab, 6월 30, 2025에 액세스, [https://madability.cs.wisc.edu/wp-content/uploads/sites/2288/2024/10/CookAR-compressed.pdf](https://madability.cs.wisc.edu/wp-content/uploads/sites/2288/2024/10/CookAR-compressed.pdf)  
19. Cooktop Sensing Based on a YOLO Object Detection Algorithm \- MDPI, 6월 30, 2025에 액세스, [https://www.mdpi.com/1424-8220/23/5/2780](https://www.mdpi.com/1424-8220/23/5/2780)  
20. Cooktop Sensing Based on a YOLO Object Detection Algorithm \- ResearchGate, 6월 30, 2025에 액세스, [https://www.researchgate.net/publication/369006334\_Cooktop\_Sensing\_Based\_on\_a\_YOLO\_Object\_Detection\_Algorithm](https://www.researchgate.net/publication/369006334_Cooktop_Sensing_Based_on_a_YOLO_Object_Detection_Algorithm)  
21. clamytoe/kitchenware\_classifier: Classifies kitchen stuff items into 6 categories: cups, glasses, plates, spoons, forks and knives \- GitHub, 6월 30, 2025에 액세스, [https://github.com/clamytoe/kitchenware\_classifier](https://github.com/clamytoe/kitchenware_classifier)  
22. Nabeel965/kitchen\_utensils\_detection: This jupyter ... \- GitHub, 6월 30, 2025에 액세스, [https://github.com/Nabeel965/kitchen\_utensils\_detection\_using\_DL](https://github.com/Nabeel965/kitchen_utensils_detection_using_DL)  
23. sabinamanafli/Yolov3\_Kitchen\_Utensils: Object Detection Project to detect some kitchen utensils using Yolov3 \- GitHub, 6월 30, 2025에 액세스, [https://github.com/sabinamanafli/Yolov3\_Kitchen\_Utensils](https://github.com/sabinamanafli/Yolov3_Kitchen_Utensils)  
24. kitchen-object-detection Object Detection Dataset by kitchenobjectdetection \- Roboflow Universe, 6월 30, 2025에 액세스, [https://universe.roboflow.com/kitchenobjectdetection/kitchen-object-detection-acyvk](https://universe.roboflow.com/kitchenobjectdetection/kitchen-object-detection-acyvk)  
25. The Food Recognition Benchmark: Using Deep Learning to Recognize Food in Images \- PMC, 6월 30, 2025에 액세스, [https://pmc.ncbi.nlm.nih.gov/articles/PMC9121091/](https://pmc.ncbi.nlm.nih.gov/articles/PMC9121091/)  
26. The Food Recognition Benchmark: Using Deep Learning to Recognize Food in Images \- Frontiers, 6월 30, 2025에 액세스, [https://www.frontiersin.org/journals/nutrition/articles/10.3389/fnut.2022.875143/full](https://www.frontiersin.org/journals/nutrition/articles/10.3389/fnut.2022.875143/full)  
27. Food Recognition 2022 \- Kaggle, 6월 30, 2025에 액세스, [https://www.kaggle.com/datasets/sainikhileshreddy/food-recognition-2022](https://www.kaggle.com/datasets/sainikhileshreddy/food-recognition-2022)  
28. marcus-suresh/PyTorch\_food\_recognition: This repository contains the code for applying a PyTorch-centric CV-based ingredients recognition application through multi-label learning. \- GitHub, 6월 30, 2025에 액세스, [https://github.com/marcus-suresh/PyTorch\_food\_recognition](https://github.com/marcus-suresh/PyTorch_food_recognition)  
29. SNAPMe: A Benchmark Dataset of Food Photos with Food Records for Evaluation of Computer Vision Algorithms in the Context of Dietary Assessment \- Ag Data Commons, 6월 30, 2025에 액세스, [https://agdatacommons.nal.usda.gov/articles/dataset/SNAPMe\_A\_Benchmark\_Dataset\_of\_Food\_Photos\_with\_Food\_Records\_for\_Evaluation\_of\_Computer\_Vision\_Algorithms\_in\_the\_Context\_of\_Dietary\_Assessment/24856449](https://agdatacommons.nal.usda.gov/articles/dataset/SNAPMe_A_Benchmark_Dataset_of_Food_Photos_with_Food_Records_for_Evaluation_of_Computer_Vision_Algorithms_in_the_Context_of_Dietary_Assessment/24856449)  
30. From the Kitchen to the Lab: Why Cooking Became AI's Favorite Dish | by Jason Corso, 6월 30, 2025에 액세스, [https://medium.com/@jasoncorso/from-the-kitchen-to-the-lab-why-cooking-became-ais-favorite-dish-2c089017c12a](https://medium.com/@jasoncorso/from-the-kitchen-to-the-lab-why-cooking-became-ais-favorite-dish-2c089017c12a)  
31. FineTea: A Novel Fine-Grained Action Recognition Video Dataset for Tea Ceremony Actions, 6월 30, 2025에 액세스, [https://www.mdpi.com/2313-433X/10/9/216](https://www.mdpi.com/2313-433X/10/9/216)  
32. Brain-inspired multimodal motion and fine-grained action recognition \- PMC, 6월 30, 2025에 액세스, [https://pmc.ncbi.nlm.nih.gov/articles/PMC11802800/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11802800/)  
33. How Do You Do It? Fine-Grained Action Understanding With Pseudo-Adverbs \- CVF Open Access, 6월 30, 2025에 액세스, [https://openaccess.thecvf.com/content/CVPR2022/papers/Doughty\_How\_Do\_You\_Do\_It\_Fine-Grained\_Action\_Understanding\_With\_Pseudo-Adverbs\_CVPR\_2022\_paper.pdf](https://openaccess.thecvf.com/content/CVPR2022/papers/Doughty_How_Do_You_Do_It_Fine-Grained_Action_Understanding_With_Pseudo-Adverbs_CVPR_2022_paper.pdf)  
34. Fine-grained action retrieval from video \- Naver Labs Europe, 6월 30, 2025에 액세스, [https://europe.naverlabs.com/blog/fine-grained-action-retrieval-from-video/](https://europe.naverlabs.com/blog/fine-grained-action-retrieval-from-video/)  
35. EPIC-KITCHENS Dataset, 6월 30, 2025에 액세스, [https://epic-kitchens.github.io/](https://epic-kitchens.github.io/)  
36. rogergranada/dataset-annotation \- GitHub, 6월 30, 2025에 액세스, [https://github.com/rogergranada/dataset-annotation](https://github.com/rogergranada/dataset-annotation)  
37. MPII Cooking Activities Dataset \- Max Planck Institute for Informatics, 6월 30, 2025에 액세스, [https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-activities-dataset](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-activities-dataset)  
38. A Database for Fine Grained Activity Detection of Cooking Activities \- CNRS, 6월 30, 2025에 액세스, [https://projet.liris.cnrs.fr/imagine/pub/proceedings/CVPR2012/data/papers/151\_P2A-01.pdf](https://projet.liris.cnrs.fr/imagine/pub/proceedings/CVPR2012/data/papers/151_P2A-01.pdf)  
39. YouCook Dataset \- Papers With Code, 6월 30, 2025에 액세스, [https://paperswithcode.com/dataset/youcook](https://paperswithcode.com/dataset/youcook)  
40. COM Kitchens: An Unedited Overhead-view Video Dataset as a Vision-Language Benchmark \- arXiv, 6월 30, 2025에 액세스, [https://arxiv.org/html/2408.02272v1](https://arxiv.org/html/2408.02272v1)  
41. Continuous Object State Recognition for Cooking Robots Using Pre-Trained Vision-Language Models and Black-box Optimization \- arXiv, 6월 30, 2025에 액세스, [https://arxiv.org/html/2403.08239v1](https://arxiv.org/html/2403.08239v1)  
42. Real-World Cooking Robot System from Recipes Based on Food State Recognition Using Foundation Models and PDDL \- arXiv, 6월 30, 2025에 액세스, [https://arxiv.org/html/2410.02874v2](https://arxiv.org/html/2410.02874v2)  
43. Guide to Vision-Language Models (VLMs) \- Encord, 6월 30, 2025에 액세스, [https://encord.com/blog/vision-language-models-guide/](https://encord.com/blog/vision-language-models-guide/)  
44. What Are Vision Language Models (VLMs)? \- IBM, 6월 30, 2025에 액세스, [https://www.ibm.com/think/topics/vision-language-models](https://www.ibm.com/think/topics/vision-language-models)  
45. Introduction to Vision Language Models \- OpenCV, 6월 30, 2025에 액세스, [https://opencv.org/blog/vision-language-models/](https://opencv.org/blog/vision-language-models/)  
46. From Seeing to Understanding: LLMs Leveraging Computer Vision, 6월 30, 2025에 액세스, [https://www.edge-ai-vision.com/2025/02/from-seeing-to-understanding-llms-leveraging-computer-vision/](https://www.edge-ai-vision.com/2025/02/from-seeing-to-understanding-llms-leveraging-computer-vision/)  
47. Understanding OpenAI's CLIP model | by Szymon Palucha \- Medium, 6월 30, 2025에 액세스, [https://medium.com/@paluchasz/understanding-openais-clip-model-6b52bade3fa3](https://medium.com/@paluchasz/understanding-openais-clip-model-6b52bade3fa3)  
48. CLIP: Contrastive Language-Image Pre-Training \- viso.ai, 6월 30, 2025에 액세스, [https://viso.ai/deep-learning/clip-machine-learning/](https://viso.ai/deep-learning/clip-machine-learning/)  
49. A Comprehensive Guide to OpenAI's CLIP Model \- TiDB, 6월 30, 2025에 액세스, [https://www.pingcap.com/article/a-comprehensive-guide-to-openais-clip-model/](https://www.pingcap.com/article/a-comprehensive-guide-to-openais-clip-model/)  
50. The Ultimate Guide to Prompt Engineering in 2025 | Lakera – Protecting AI teams that disrupt the world., 6월 30, 2025에 액세스, [https://www.lakera.ai/blog/prompt-engineering-guide](https://www.lakera.ai/blog/prompt-engineering-guide)  
51. Prompt engineering: A guide to improving LLM performance \- CircleCI, 6월 30, 2025에 액세스, [https://circleci.com/blog/prompt-engineering/](https://circleci.com/blog/prompt-engineering/)  
52. ChatGPT Prompt of the Day: "PROMPT MASTERY COACH: Your Ultimate AI Guide to Crafting Perfect Prompts" : r/ChatGPTPromptGenius \- Reddit, 6월 30, 2025에 액세스, [https://www.reddit.com/r/ChatGPTPromptGenius/comments/1i6y1sr/chatgpt\_prompt\_of\_the\_day\_prompt\_mastery\_coach/](https://www.reddit.com/r/ChatGPTPromptGenius/comments/1i6y1sr/chatgpt_prompt_of_the_day_prompt_mastery_coach/)  
53. Demystifying Instruction Fine-Tuning in Large Language Models | by Nandini Lokesh Reddy, 6월 30, 2025에 액세스, [https://medium.com/@nandinilreddy/demystifying-instruction-fine-tuning-in-large-language-models-0df732a0cec2](https://medium.com/@nandinilreddy/demystifying-instruction-fine-tuning-in-large-language-models-0df732a0cec2)  
54. What Is Instruction Tuning? | IBM, 6월 30, 2025에 액세스, [https://www.ibm.com/think/topics/instruction-tuning](https://www.ibm.com/think/topics/instruction-tuning)  
55. Fine-tune a large language model (LLM) using prompt instructions \- Amazon SageMaker AI, 6월 30, 2025에 액세스, [https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-fine-tuning-instruction-based.html](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-fine-tuning-instruction-based.html)  
56. Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models \- arXiv, 6월 30, 2025에 액세스, [https://arxiv.org/html/2505.11326v1](https://arxiv.org/html/2505.11326v1)  
57. \[2503.05064\] Perceiving, Reasoning, Adapting: A Dual-Layer Framework for VLM-Guided Precision Robotic Manipulation \- arXiv, 6월 30, 2025에 액세스, [https://arxiv.org/abs/2503.05064](https://arxiv.org/abs/2503.05064)  
58. Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning \- CVPR 2025, 6월 30, 2025에 액세스, [https://cvpr.thecvf.com/virtual/2025/poster/33205](https://cvpr.thecvf.com/virtual/2025/poster/33205)  
59. Fine-grained Control of Generative Data Augmentation in IoT Sensing \- OpenReview, 6월 30, 2025에 액세스, [https://openreview.net/forum?id=ZCygNDMIII\&referrer=%5Bthe%20profile%20of%20Yigong%20Hu%5D(%2Fprofile%3Fid%3D\~Yigong\_Hu1)](https://openreview.net/forum?id=ZCygNDMIII&referrer=%5Bthe+profile+of+Yigong+Hu%5D\(/profile?id%3D~Yigong_Hu1\))  
60. A Feature-space Multimodal Data Augmentation Technique for Text-video Retrieval \- arXiv, 6월 30, 2025에 액세스, [https://arxiv.org/pdf/2208.02080](https://arxiv.org/pdf/2208.02080)  
61. Exploring Temporally Dynamic Data Augmentation for Video Recognition \- arXiv, 6월 30, 2025에 액세스, [https://arxiv.org/abs/2206.15015](https://arxiv.org/abs/2206.15015)  
62. Leveraging Unreal Engine 5 for Synthetic Data Generation \- YouTube, 6월 30, 2025에 액세스, [https://www.youtube.com/watch?v=t5dgMfF8xSE](https://www.youtube.com/watch?v=t5dgMfF8xSE)  
63. Creating Synthetic Data Using Unity | Sliced Bread Animation, 6월 30, 2025에 액세스, [https://sbanimation.com/creating-synthetic-data-using-unity/](https://sbanimation.com/creating-synthetic-data-using-unity/)  
64. Synthetic Training Dataset with Unity \- Valohai, 6월 30, 2025에 액세스, [https://valohai.com/blog/synthetic-training-dataset-generation-with-unity/](https://valohai.com/blog/synthetic-training-dataset-generation-with-unity/)  
65. Synthetic Data Generation with Unity 3D and Unreal Engine for Construction Hazard Scenarios: A Comparative Analysis \- ResearchGate, 6월 30, 2025에 액세스, [https://www.researchgate.net/publication/382888381\_Synthetic\_Data\_Generation\_with\_Unity\_3D\_and\_Unreal\_Engine\_for\_Construction\_Hazard\_Scenarios\_A\_Comparative\_Analysis](https://www.researchgate.net/publication/382888381_Synthetic_Data_Generation_with_Unity_3D_and_Unreal_Engine_for_Construction_Hazard_Scenarios_A_Comparative_Analysis)  
66. replicAnt: a pipeline for generating annotated images of animals in complex environments using Unreal Engine \- PubMed Central, 6월 30, 2025에 액세스, [https://pmc.ncbi.nlm.nih.gov/articles/PMC10632501/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10632501/)  
67. Best Practices For Creating An AI Infrastructure Architecture For Modern Data Systems, 6월 30, 2025에 액세스, [https://www.copado.com/resources/blog/best-practices-for-creating-an-ai-infrastructure-architecture-for-modern-data-systems](https://www.copado.com/resources/blog/best-practices-for-creating-an-ai-infrastructure-architecture-for-modern-data-systems)  
68. Deploying an Efficient Vision-Language Model on Mobile Devices, 6월 30, 2025에 액세스, [https://www.edge-ai-vision.com/2025/05/deploying-an-efficient-vision-language-model-on-mobile-devices/](https://www.edge-ai-vision.com/2025/05/deploying-an-efficient-vision-language-model-on-mobile-devices/)  
69. Machine learning on mobile devices: 3 steps for deploying ML in your apps \- Heartbeat, 6월 30, 2025에 액세스, [https://heartbeat.comet.ml/machine-learning-on-mobile-devices-3-steps-for-deploying-it-in-your-apps-48a0a24364a8](https://heartbeat.comet.ml/machine-learning-on-mobile-devices-3-steps-for-deploying-it-in-your-apps-48a0a24364a8)  
70. How to bring your AI Model to Android devices, 6월 30, 2025에 액세스, [https://android-developers.googleblog.com/2024/10/bring-your-ai-model-to-android-devices.html](https://android-developers.googleblog.com/2024/10/bring-your-ai-model-to-android-devices.html)  
71. Core ML | Apple Developer Documentation, 6월 30, 2025에 액세스, [https://developer.apple.com/documentation/coreml/](https://developer.apple.com/documentation/coreml/)  
72. Core ML and Vision Tutorial: On-device training on iOS \- Kodeco, 6월 30, 2025에 액세스, [https://www.kodeco.com/7960296-core-ml-and-vision-tutorial-on-device-training-on-ios](https://www.kodeco.com/7960296-core-ml-and-vision-tutorial-on-device-training-on-ios)  
73. Optimize your Core ML usage \- WWDC22 \- Videos \- Apple Developer, 6월 30, 2025에 액세스, [https://developer.apple.com/videos/play/wwdc2022/10027/](https://developer.apple.com/videos/play/wwdc2022/10027/)  
74. How to feed video buffer data to CoreML? : r/swift \- Reddit, 6월 30, 2025에 액세스, [https://www.reddit.com/r/swift/comments/11pqz5x/how\_to\_feed\_video\_buffer\_data\_to\_coreml/](https://www.reddit.com/r/swift/comments/11pqz5x/how_to_feed_video_buffer_data_to_coreml/)  
75. Deploy machine learning and AI models on-device with Core ML \- WWDC24 \- Videos, 6월 30, 2025에 액세스, [https://developer.apple.com/videos/play/wwdc2024/10161/](https://developer.apple.com/videos/play/wwdc2024/10161/)  
76. Use a custom TensorFlow Lite model on Android | Firebase ML, 6월 30, 2025에 액세스, [https://firebase.google.com/docs/ml/android/use-custom-models](https://firebase.google.com/docs/ml/android/use-custom-models)  
77. LiteRT for Android | Google AI Edge \- Gemini API, 6월 30, 2025에 액세스, [https://ai.google.dev/edge/litert/android](https://ai.google.dev/edge/litert/android)  
78. MediaPipe: Real-Time Computer Vision Reimagined | by Prince Kushwaha \- Medium, 6월 30, 2025에 액세스, [https://medium.com/@p4prince2/mediapipe-real-time-computer-vision-reimagined-d22bcb173143](https://medium.com/@p4prince2/mediapipe-real-time-computer-vision-reimagined-d22bcb173143)  
79. google-ai-edge/mediapipe: Cross-platform, customizable ML solutions for live and streaming media. \- GitHub, 6월 30, 2025에 액세스, [https://github.com/google-ai-edge/mediapipe](https://github.com/google-ai-edge/mediapipe)  
80. TensorFlow Lite Android Example \[Beginners\] | Analytics Vidhya \- Medium, 6월 30, 2025에 액세스, [https://medium.com/analytics-vidhya/tensorflow-lite-on-android-2f57267f7297](https://medium.com/analytics-vidhya/tensorflow-lite-on-android-2f57267f7297)  
81. Gesture recognition task guide | Google AI Edge \- Gemini API, 6월 30, 2025에 액세스, [https://ai.google.dev/edge/mediapipe/solutions/vision/gesture\_recognizer](https://ai.google.dev/edge/mediapipe/solutions/vision/gesture_recognizer)  
82. Gesture recognition guide for Android | Google AI Edge \- Gemini API, 6월 30, 2025에 액세스, [https://ai.google.dev/edge/mediapipe/solutions/vision/gesture\_recognizer/android](https://ai.google.dev/edge/mediapipe/solutions/vision/gesture_recognizer/android)  
83. Real-Time 3D Object Detection on Mobile Devices with MediaPipe \- Google Research, 6월 30, 2025에 액세스, [https://research.google/blog/real-time-3d-object-detection-on-mobile-devices-with-mediapipe/](https://research.google/blog/real-time-3d-object-detection-on-mobile-devices-with-mediapipe/)  
84. LLM Inference guide for Android | Google AI Edge \- Gemini API, 6월 30, 2025에 액세스, [https://ai.google.dev/edge/mediapipe/solutions/genai/llm\_inference/android](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/android)  
85. Jump-to-Recipe: An LLM-powered AI assistant for recipes / benlafreniere.ca, 6월 30, 2025에 액세스, [https://www.benlafreniere.ca/blog/jump-to-recipe.html](https://www.benlafreniere.ca/blog/jump-to-recipe.html)  
86. “Culinary Dimensions: Discovering Himachali Flavours with Augmented Reality” \- RSIS International, 6월 30, 2025에 액세스, [https://rsisinternational.org/journals/ijriss/Digital-Library/volume-9-issue-14/629-637.pdf](https://rsisinternational.org/journals/ijriss/Digital-Library/volume-9-issue-14/629-637.pdf)